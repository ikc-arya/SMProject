{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Audio file list ---\n",
    "audio_files = [\n",
    "    \"../data/raw/video/Muppets-02-01-01.wav\",\n",
    "    \"../data/raw/video/Muppets-02-04-04.wav\",\n",
    "    \"../data/raw/video/Muppets-03-04-03.wav\"\n",
    "]\n",
    "\n",
    "# --- 2. Frame and feature settings ---\n",
    "SR = 22050 \n",
    "HOP_LENGTH = int(0.040 * SR) # 40 ms hop size\n",
    "N_FFT = 2048 \n",
    "TARGET_COLUMNS = ['Kermit', 'StatlerWaldorf', 'Fozzie Bear'] # Add this here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üé¨ Processing file: Muppets-03-04-03.wav\n",
      "‚úÖ Success. Total frames: 38500\n",
      "   Feature dimensionality: (41, 38500)\n",
      "\n",
      "üé¨ Processing file: Muppets-02-04-04.wav\n",
      "‚úÖ Success. Total frames: 38708\n",
      "   Feature dimensionality: (41, 38708)\n",
      "\n",
      "üé¨ Processing file: Muppets-02-01-01.wav\n",
      "‚úÖ Success. Total frames: 38683\n",
      "   Feature dimensionality: (41, 38683)\n",
      "\n",
      "--- All frame-level features successfully extracted! ---\n"
     ]
    }
   ],
   "source": [
    "# --- PROJECT CONSTANTS ---\n",
    "TARGET_COLUMNS = ['Kermit', 'StatlerWaldorf', 'Fozzie Bear'] \n",
    "SR = 22050 \n",
    "HOP_LENGTH = int(0.040 * SR) # 40 ms hop size\n",
    "N_FFT = 2048 # FFT window size\n",
    "\n",
    "# 1. Audio file list \n",
    "audio_files = [\n",
    "    \"../data/raw/video/Muppets-03-04-03.wav\",\n",
    "    \"../data/raw/video/Muppets-02-04-04.wav\",\n",
    "    \"../data/raw/video/Muppets-02-01-01.wav\"\n",
    "]\n",
    "\n",
    "# 2. Function for frame-level feature extraction \n",
    "def extract_features_from_file(audio_path, sr, hop_length, n_fft):\n",
    "    \"\"\"Loads audio and extracts frame-level MFCCs, F0, and Spectral Centroid.\"\"\"\n",
    "    \n",
    "    if not os.path.exists(audio_path):\n",
    "        print(f\"‚ùå File not found: {audio_path}. Skipping.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"\\nüé¨ Processing file: {os.path.basename(audio_path)}\")\n",
    "    \n",
    "    # Load audio data\n",
    "    audio_data, _ = librosa.load(audio_path, sr=sr)\n",
    "    \n",
    "    # Feature extraction \n",
    "    \n",
    "    # MFCCs (13 coefficients + 1st and 2nd derivatives = 39 features)\n",
    "    mfccs = librosa.feature.mfcc(y=audio_data, sr=sr, n_mfcc=13, n_fft=n_fft, hop_length=hop_length)\n",
    "    mfccs_delta = librosa.feature.delta(mfccs, order=1)\n",
    "    mfccs_delta2 = librosa.feature.delta(mfccs, order=2)\n",
    "    mfccs_full = np.concatenate((mfccs, mfccs_delta, mfccs_delta2), axis=0)\n",
    "    \n",
    "    # Spectral Centroid (1 feature)\n",
    "    spectral_centroids = librosa.feature.spectral_centroid(y=audio_data, sr=sr, n_fft=n_fft, hop_length=hop_length)[0]\n",
    "    \n",
    "    # F0 (Pitch) (1 feature)\n",
    "    f0, voiced_flag, _ = librosa.pyin(\n",
    "        y=audio_data, sr=sr, \n",
    "        fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C5'), \n",
    "        hop_length=hop_length,\n",
    "    )\n",
    "\n",
    "    # --- 3. Concatenate all frame-level features (Total 41 features) ---\n",
    "    \n",
    "    # check that all arrays have the same number of frames\n",
    "    min_frames = min(mfccs_full.shape[1], spectral_centroids.shape[0], f0.shape[0])\n",
    "\n",
    "    # Combine into a single matrix (features are along the rows)\n",
    "    all_features_frame_level = np.concatenate([\n",
    "        mfccs_full[:, :min_frames],\n",
    "        spectral_centroids[:min_frames][np.newaxis, :], \n",
    "        f0[:min_frames][np.newaxis, :]                  \n",
    "    ], axis=0) \n",
    "\n",
    "    print(f\"‚úÖ Success. Total frames: {min_frames}\")\n",
    "    print(f\"   Feature dimensionality: {all_features_frame_level.shape}\") # (41, N_frames)\n",
    "    \n",
    "    return {\n",
    "        'episode': os.path.basename(audio_path),\n",
    "        'features': all_features_frame_level,\n",
    "        'voiced_flag': voiced_flag[:min_frames]\n",
    "    }\n",
    "\n",
    "# 4. Run processing and save results\n",
    "all_episodes_data = []\n",
    "\n",
    "for audio_file_path in audio_files:\n",
    "    result = extract_features_from_file(audio_file_path, SR, HOP_LENGTH, N_FFT)\n",
    "    if result is not None:\n",
    "        all_episodes_data.append(result)\n",
    "\n",
    "print(\"\\n--- All frame-level features successfully extracted! ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pandas \n",
    "#pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded GT: Ground_Truth_New_01.xlsx\n",
      "‚úÖ Loaded GT: Ground_Truth_New_03.xlsx\n",
      "‚úÖ Loaded GT: Ground_Truth_New_04.xlsx\n",
      "\n",
      "Total frames in combined GT: 115885\n"
     ]
    }
   ],
   "source": [
    "# Load Ground Truth (GT) Data ---\n",
    "\n",
    "gt_files = [\n",
    "    \"/Users/iana/projects/dicom-dev/SMProject/data/muppets-gt-2025wt/Ground_Truth_New_01.xlsx\",\n",
    "    \"/Users/iana/projects/dicom-dev/SMProject/data/muppets-gt-2025wt/Ground_Truth_New_03.xlsx\",\n",
    "    \"/Users/iana/projects/dicom-dev/SMProject/data/muppets-gt-2025wt/Ground_Truth_New_04.xlsx\"\n",
    "]\n",
    "\n",
    "all_gt_data = []\n",
    "\n",
    "for file_path in gt_files:\n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_excel(file_path)\n",
    "        # add to the list for concatenation\n",
    "        all_gt_data.append(df)\n",
    "        print(f\"‚úÖ Loaded GT: {os.path.basename(file_path)}\")\n",
    "    else:\n",
    "        print(f\"‚ùå GT file not found: {os.path.basename(file_path)}\")\n",
    "\n",
    "gt_df = pd.concat(all_gt_data, ignore_index=True)\n",
    "\n",
    "print(f\"\\nTotal frames in combined GT: {len(gt_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting to train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7y/wgwt8n3n1ss_t788msvzbnl80000gn/T/ipykernel_44842/3899089193.py:32: RuntimeWarning: Mean of empty slice\n",
      "  mu = np.nanmean(window_features, axis=0)\n",
      "/Users/iana/Library/Python/3.9/lib/python/site-packages/numpy/lib/_nanfunctions_impl.py:2035: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/var/folders/7y/wgwt8n3n1ss_t788msvzbnl80000gn/T/ipykernel_44842/3899089193.py:32: RuntimeWarning: Mean of empty slice\n",
      "  mu = np.nanmean(window_features, axis=0)\n",
      "/Users/iana/Library/Python/3.9/lib/python/site-packages/numpy/lib/_nanfunctions_impl.py:2035: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/var/folders/7y/wgwt8n3n1ss_t788msvzbnl80000gn/T/ipykernel_44842/3899089193.py:32: RuntimeWarning: Mean of empty slice\n",
      "  mu = np.nanmean(window_features, axis=0)\n",
      "/Users/iana/Library/Python/3.9/lib/python/site-packages/numpy/lib/_nanfunctions_impl.py:2035: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 11575 aggregated event vectors.\n",
      "\n",
      "--- Train/Test Split Summary ---\n",
      "Training set (Train): 7712 samples\n",
      "Test set (Test): 3863 samples\n",
      "Test proportion: 33.4% of total data.\n"
     ]
    }
   ],
   "source": [
    "#  SETTINGS AND CONSTANTS \n",
    "WINDOW_SIZE = 50      # Window size in frames (50 frames * 40ms/frame = 2.0 seconds)\n",
    "HOP_SIZE = 10         # Hop size in frames (10 frames * 40ms/frame = 0.4 seconds)\n",
    "SR = 22050\n",
    "TOTAL_FEATURE_COUNT = 41 # (MFCCs(39) + Centroid(1) + F0(1))\n",
    "TARGET_COLUMNS = ['Kermit', 'StatlerWaldorf', 'Fozzie Bear']\n",
    "TEST_EPISODE_ID = 'Muppets-02-01-01'\n",
    "TEST_TIME_START_SECONDS = 725 # 12 minutes 5 seconds cus this is the part of the video where all the caracters were visible and anough scenes \n",
    "\n",
    "#  1. AGGREGATION AND SLIDING WINDOW \n",
    "aggregated_dataset = []\n",
    "episode_start_frame = 0 \n",
    "\n",
    "for episode_data in all_episodes_data:\n",
    "    features = episode_data['features'].T # (N_frames, 41)\n",
    "    n_frames_episode = features.shape[0]\n",
    "    \n",
    "    # Slice frame-level labels for the current episode\n",
    "    labels_episode = gt_df[TARGET_COLUMNS].values[episode_start_frame : episode_start_frame + n_frames_episode]\n",
    "    episode_id = episode_data['episode'].replace('.wav', '')\n",
    "    \n",
    "    min_length = min(n_frames_episode, len(labels_episode))\n",
    "    features = features[:min_length, :]\n",
    "    labels_episode = labels_episode[:min_length, :]\n",
    "\n",
    "    # Sliding Window Loop\n",
    "    for i in range(0, min_length - WINDOW_SIZE + 1, HOP_SIZE):\n",
    "        window_features = features[i : i + WINDOW_SIZE, :]\n",
    "        window_labels = labels_episode[i : i + WINDOW_SIZE, :]\n",
    "        \n",
    "        # Feature Aggregation (mu and sigma) ---\n",
    "        mu = np.nanmean(window_features, axis=0)\n",
    "        sigma = np.nanstd(window_features, axis=0)\n",
    "        \n",
    "        # Concatenate mean and standard deviation (82 features total)\n",
    "        feature_vector = np.concatenate([mu, sigma]) \n",
    "        \n",
    "        # Label Aggregation (Majority Voting) \n",
    "        presence_counts = np.sum(window_labels, axis=0)\n",
    "        dominant_index = np.argmax(presence_counts)\n",
    "        dominant_count = presence_counts[dominant_index]\n",
    "        \n",
    "        if dominant_count > (WINDOW_SIZE / 2):\n",
    "            final_label = TARGET_COLUMNS[dominant_index]\n",
    "        else:\n",
    "            final_label = \"None/Background\"\n",
    "            \n",
    "        # Calculate Start Time (in seconds)\n",
    "        T_start = (episode_start_frame + i) * 0.040 # Time start of the frame\n",
    "        \n",
    "    \n",
    "        aggregated_dataset.append({\n",
    "            'features': feature_vector,\n",
    "            'label': final_label,\n",
    "            'episode_id': episode_id,         \n",
    "            'time_start': T_start             \n",
    "        })\n",
    "        \n",
    "    episode_start_frame += n_frames_episode\n",
    "\n",
    "print(f\"\\nCreated {len(aggregated_dataset)} aggregated event vectors.\")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Step 2: Final dataframe creation and Train/Test splits\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# 1. DataFrame\n",
    "final_df = pd.DataFrame(aggregated_dataset)\n",
    "\n",
    "# 2. Expand feature vector into separate columns\n",
    "feature_names = [f'{stat}_{i}' for stat in ['mu', 'sigma'] for i in range(TOTAL_FEATURE_COUNT)]\n",
    "feature_df = final_df['features'].apply(pd.Series)\n",
    "feature_df.columns = feature_names\n",
    "\n",
    "# Concatenate features with metadata\n",
    "final_df = pd.concat([final_df.drop('features', axis=1), feature_df], axis=1)\n",
    "\n",
    "\n",
    "# 3. Time-based Split condition\n",
    "test_condition = (final_df['episode_id'].str.contains(TEST_EPISODE_ID)) & \\\n",
    "                 (final_df['time_start'] >= TEST_TIME_START_SECONDS)\n",
    "\n",
    "X = final_df.drop(['label', 'episode_id', 'time_start'], axis=1) # Features\n",
    "y = final_df['label'] \n",
    "\n",
    "X_test = X[test_condition]\n",
    "y_test = y[test_condition]\n",
    "\n",
    "X_train = X[~test_condition] \n",
    "y_train = y[~test_condition]\n",
    "\n",
    "print(f\"\\n--- Train/Test Split Summary ---\")\n",
    "print(f\"Training set (Train): {len(X_train)} samples\")\n",
    "print(f\"Test set (Test): {len(X_test)} samples\")\n",
    "print(f\"Test proportion: {len(X_test) / len(final_df):.1%} of total data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Class Distribution (Train Set):\n",
      "label\n",
      "None/Background    4599\n",
      "Kermit             2055\n",
      "Fozzie Bear         791\n",
      "StatlerWaldorf      267\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Percentage Distribution:\n",
      "label\n",
      "None/Background    59.634336\n",
      "Kermit             26.646784\n",
      "Fozzie Bear        10.256743\n",
      "StatlerWaldorf      3.462137\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Checking the class distribution in the splits\n",
    "class_counts = y_train.value_counts()\n",
    "total_train = len(y_train)\n",
    "\n",
    "print(\"\\nüìä Class Distribution (Train Set):\")\n",
    "print(class_counts)\n",
    "print(\"\\nPercentage Distribution:\")\n",
    "print((class_counts / total_train) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Class Distribution in the TEST Set:\n",
      "label\n",
      "None/Background    2433\n",
      "Kermit             1214\n",
      "Fozzie Bear         109\n",
      "StatlerWaldorf      107\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Percentage Distribution:\n",
      "label\n",
      "None/Background    62.982138\n",
      "Kermit             31.426353\n",
      "Fozzie Bear         2.821641\n",
      "StatlerWaldorf      2.769868\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Test split class distribution\n",
    "\n",
    "class_counts_test = y_test.value_counts()\n",
    "total_test = len(y_test)\n",
    "\n",
    "print(\"\\nüìä Class Distribution in the TEST Set:\")\n",
    "print(class_counts_test)\n",
    "print(\"\\nPercentage Distribution:\")\n",
    "print((class_counts_test / total_test) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) NaN Imputation completed.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Imputation (Filling NaN values)\n",
    "\n",
    "# 1. Initialize and FIT Imputer ONLY on X_train\n",
    "# Strategy: 'mean' (using the mean of each column from the training set)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "imputer.fit(X_train) \n",
    "\n",
    "# 2. APPLY Imputer to X_train and X_test\n",
    "X_train_imputed = imputer.transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "X_train_imputed = pd.DataFrame(X_train_imputed, columns=X_train.columns, index=X_train.index)\n",
    "X_test_imputed = pd.DataFrame(X_test_imputed, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "print(\"1) NaN Imputation completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2) Normalization completed. Data is ready for training.\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Normalization (Feature Scaling)\n",
    "\n",
    "# 1. Initialize and FIT Scaler ONLY on X_train_imputed\n",
    "# This fits the scaler to calculate mean (mu) and standard deviation (sigma) only from the training data.\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_imputed) \n",
    "\n",
    "# 2. APPLY Scaler to both train and test sets\n",
    "# The test data is scaled using the mu and sigma derived ONLY from the training data (no data leakage).\n",
    "X_train_scaled = scaler.transform(X_train_imputed)\n",
    "X_test_scaled = scaler.transform(X_test_imputed)\n",
    "\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "print(\"2) Normalization completed. Data is ready for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- SVC Results (Class Weighted) ---\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    Fozzie Bear       0.06      0.79      0.12       109\n",
      "         Kermit       0.39      0.43      0.41      1214\n",
      "None/Background       0.60      0.16      0.25      2433\n",
      " StatlerWaldorf       0.01      0.06      0.02       107\n",
      "\n",
      "       accuracy                           0.26      3863\n",
      "      macro avg       0.26      0.36      0.20      3863\n",
      "   weighted avg       0.50      0.26      0.29      3863\n",
      "\n",
      "\n",
      "--- Random Forest Results (Class Weighted) ---\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    Fozzie Bear       0.00      0.00      0.00       109\n",
      "         Kermit       0.36      0.04      0.07      1214\n",
      "None/Background       0.63      0.97      0.76      2433\n",
      " StatlerWaldorf       0.00      0.00      0.00       107\n",
      "\n",
      "       accuracy                           0.62      3863\n",
      "      macro avg       0.25      0.25      0.21      3863\n",
      "   weighted avg       0.51      0.62      0.50      3863\n",
      "\n",
      "\n",
      "--- k-NN Results (Unweighted) ---\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    Fozzie Bear       0.02      0.06      0.03       109\n",
      "         Kermit       0.33      0.29      0.31      1214\n",
      "None/Background       0.63      0.64      0.63      2433\n",
      " StatlerWaldorf       0.00      0.00      0.00       107\n",
      "\n",
      "       accuracy                           0.50      3863\n",
      "      macro avg       0.25      0.25      0.24      3863\n",
      "   weighted avg       0.50      0.50      0.50      3863\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Since the class where characters are not present in the video (None/Background) is the dominant\n",
    "# we need to do imbalance compensation while training models\n",
    "\n",
    "# Training SVC with class imbalance compensation (Linear Kernel) \n",
    "# class_weight='balanced' automatically adjusts weights inversely proportional to class frequencies.\n",
    "svc_model = SVC(kernel='linear', C=1, class_weight='balanced', random_state=42) \n",
    "svc_model.fit(X_train_scaled, y_train)\n",
    "y_pred_svc = svc_model.predict(X_test_scaled)\n",
    "print(\"\\n--- SVC Results (Class Weighted) ---\")\n",
    "print(classification_report(y_test, y_pred_svc, zero_division=0))\n",
    "\n",
    "# Training Random Forest with class imbalance compensation \n",
    "rf_model = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test_scaled)\n",
    "print(\"\\n--- Random Forest Results (Class Weighted) ---\")\n",
    "print(classification_report(y_test, y_pred_rf, zero_division=0))\n",
    "\n",
    "# Training k-NN (Unweighted) \n",
    "# k-NN does not natively support class weighting.\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\n",
    "knn_model.fit(X_train_scaled, y_train)\n",
    "y_pred_knn = knn_model.predict(X_test_scaled)\n",
    "print(\"\\n--- k-NN Results (Unweighted) ---\")\n",
    "print(classification_report(y_test, y_pred_knn, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting GridSearchCV for SVC Tuning ---\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   3.8s\n",
      "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   3.8s\n",
      "[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   3.8s\n",
      "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   3.8s\n",
      "[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   3.9s\n",
      "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   3.9s\n",
      "[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   4.0s\n",
      "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   3.9s\n",
      "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   4.0s\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   4.3s\n",
      "[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   2.8s\n",
      "[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   2.9s\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   2.9s\n",
      "[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   3.1s\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   3.0s\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   3.0s\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   3.7s\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   3.8s\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   3.9s\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   3.8s\n",
      "[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   3.0s\n",
      "[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   3.1s\n",
      "[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   2.8s\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   3.0s\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   4.0s\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   4.0s\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   2.8s\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   2.8s\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   4.2s\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   4.3s\n",
      "[CV] END .....................C=100, gamma=scale, kernel=rbf; total time=   3.0s\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   4.0s\n",
      "[CV] END .....................C=100, gamma=scale, kernel=rbf; total time=   3.2s\n",
      "[CV] END .....................C=100, gamma=scale, kernel=rbf; total time=   3.5s\n",
      "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   3.1s\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   4.3s\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   4.3s\n",
      "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   4.2s\n",
      "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   4.3s\n",
      "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   4.5s\n",
      "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   2.4s\n",
      "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   2.8s\n",
      "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   3.8s\n",
      "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   3.7s\n",
      "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   3.7s\n",
      "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   3.8s\n",
      "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   3.7s\n",
      "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   3.5s\n",
      "\n",
      " Tuning completed.\n",
      "Best parameters based on F1-weighted: {'C': 1, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "Best F1-weighted score (CV): 0.4845\n",
      "\n",
      "--- Tuned SVC (RBF) Results on Test Set ---\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    Fozzie Bear       0.00      0.00      0.00       109\n",
      "         Kermit       0.46      0.08      0.13      1214\n",
      "None/Background       0.63      0.95      0.76      2433\n",
      " StatlerWaldorf       0.00      0.00      0.00       107\n",
      "\n",
      "       accuracy                           0.62      3863\n",
      "      macro avg       0.27      0.26      0.22      3863\n",
      "   weighted avg       0.54      0.62      0.52      3863\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVC Hyperparameter Tuning: Grid Definition \n",
    "# C: Regularization parameter. Higher C means a stricter model (aims to increase Precision which is important in our case).\n",
    "# gamma: RBF kernel coefficient.\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 0.01, 0.1, 1],\n",
    "    'kernel': ['rbf']\n",
    "}\n",
    "\n",
    "# Base SVC model (keeping class_weight='balanced')\n",
    "svc_base = SVC(class_weight='balanced', random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "# Scoring metric is F1-weighted to account for class importance in imbalanced data.\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=svc_base, \n",
    "    param_grid=param_grid, \n",
    "    scoring='f1_weighted',\n",
    "    cv=3,                  \n",
    "    verbose=2,             \n",
    "    n_jobs=-1              \n",
    ")\n",
    "\n",
    "print(\"\\n--- Starting GridSearchCV for SVC Tuning ---\")\n",
    "\n",
    "# Train Grid Search (using previously scaled training data)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Reporting Tuning Results \n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "print(\"\\n Tuning completed.\")\n",
    "print(f\"Best parameters based on F1-weighted: {best_params}\")\n",
    "print(f\"Best F1-weighted score (CV): {best_score:.4f}\")\n",
    "\n",
    "# Evaluate the best model on the unseen test set\n",
    "y_pred_tuned = best_model.predict(X_test_scaled)\n",
    "\n",
    "print(\"\\n--- Tuned SVC (RBF) Results on Test Set ---\")\n",
    "print(classification_report(y_test, y_pred_tuned, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're done with the audio phase. Our best audio model (SVC) is good at finding the characters (high Recall), but it often makes mistakes (low Precision). This shows the limitation of using audio features alone.\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "To truly boost accuracy, we need to bring in visual information.\n",
    "1) Get the Visual Features (the face data your partner extracted).\n",
    "2) Combine these features with our current audio features.\n",
    "3) Retrain the Model using this combined dataset.\n",
    "\n",
    "This should dramatically improve our ability to accurately determine who is speaking. Get ready for Feature Fusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2db9860d",
   "metadata": {},
   "source": [
    "# Similarity Modeling 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd734fdb",
   "metadata": {},
   "source": [
    "## Feature Choice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efe7225",
   "metadata": {},
   "source": [
    "In SIM1 we were supposed to find frames with the characters: Kermit, StatlerWaldorf and Fozzoe Bear. \n",
    "\n",
    "**In visual domain**:\n",
    "\n",
    "- we focused on \"Kermit\" and \"Fozzie Bear\". \n",
    "- For Kermit, 'green_mask' and 'eye_blob' features aee used. He have a distinctive green color and there is a black curve in his eye blobs veside a black dot in the middle, adding these 2 up we find him.\n",
    "- As for the Fozzie Bear, his skin color is light brown and a unique texture, so 'brown_rhythm' pattern is used to locate his frames.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**In Audio Domain:**\n",
    "- Walderf and Statler do not have specific visual features but have unique voice eatures, so visual domain feature space focus more on them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf863025",
   "metadata": {},
   "source": [
    "# Multimodal Fusion: Discussion and Conclusions\n",
    "\n",
    "To overcome the limitations of unimodal approaches, we applied a late fusion strategy combining audio-based and visual-based predictions. Instead of concatenating features at an early stage, we fused the prediction scores from independently trained audio and visual models. This approach allows each modality to contribute according to its strengths and avoids issues related to feature scale mismatch and dimensionality imbalance.\n",
    "\n",
    "Importantly, the fusion strategy was designed in a character-oriented manner. For Kermit, audio information was given a higher weight, as his voice provides strong discriminative cues. For Statler & Waldorf, visual information was emphasized, since these characters are visually distinctive but difficult to separate using audio alone. This character-specific weighting reflects the actual properties of the data and leads to more interpretable results.\n",
    "\n",
    "The fusion results show a substantial improvement over both unimodal baselines. After fusion, the MAP increased to 0.76 for Kermit and 0.65 for Statler & Waldorf, with an overall MAP of approximately 0.70. This clearly demonstrates that combining modalities significantly improves detection performance and robustness.\n",
    "\n",
    "These results confirm that audio and visual modalities provide complementary information. Audio helps resolve ambiguities when a character is speaking but not clearly visible, while visual features help in scenes with background noise, overlapping speech, or silent character presence. By combining both sources, the system is able to reduce false positives and false negatives that occur in unimodal settings.\n",
    "\n",
    "# Final Remarks\n",
    "\n",
    "In conclusion, the experiments show that multimodal fusion is essential for reliable character detection in complex video material such as The Muppet Show. Audio-only and visual-only approaches each have clear limitations, but when combined in a principled way, they achieve significantly better and more balanced performance. The character-oriented fusion strategy used in this project provides an effective and interpretable solution that aligns well with the requirements and goals of SIM1."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

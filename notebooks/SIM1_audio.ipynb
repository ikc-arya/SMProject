{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Modeling 1 - Audio Domain Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Setup & paths\n",
    "\n",
    "1. Enable autoreload for local utils/ modules\n",
    "\n",
    "2. Add project root to sys.path to import utils.*\n",
    "\n",
    "3. Define constants (FPS) used for audio↔visual alignment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from utils import audio_tools as audioTools\n",
    "from utils import gt_and_modeling_dfs as prepare_df\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from utils import evaluation_tools as eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Choice of split:**\\\n",
    "> The split had to be identical with visual features extraction for the future fusion, so we use the same strategy as there: There are 4 main characters that we need to identify so the split time from a given episode is selected based on the equal (rough idea) no of apperances of the all all character in both splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FPS_TO_SAVE = 25  # to match visual\n",
    "\n",
    "EPISODES = {\n",
    "    \"Muppets-02-01-01\": {\n",
    "        \"path\": \"../data/raw/Muppets-02-01-01.avi\",\n",
    "        \"train_split_timestamp\": \"19:30\",\n",
    "        \"ground_truth_path\": \"../data/muppets-gt-2025wt/Ground_Truth_New_01.xlsx\"\n",
    "    },\n",
    "    \"Muppets-02-04-04\": {\n",
    "        \"path\": \"../data/raw/Muppets-02-04-04.avi\",\n",
    "        \"train_split_timestamp\": \"19:52\",\n",
    "        \"ground_truth_path\": \"../data/muppets-gt-2025wt/Ground_Truth_New_04.xlsx\"\n",
    "    },\n",
    "    \"Muppets-03-04-03\": {\n",
    "        \"path\": \"../data/raw/Muppets-03-04-03.avi\",\n",
    "        \"train_split_timestamp\": \"19:54\",\n",
    "        \"ground_truth_path\": \"../data/muppets-gt-2025wt/Ground_Truth_New_03.xlsx\"\n",
    "    }\n",
    "}\n",
    "\n",
    "EPISODE_NAME_TO_VIDEO_ID = {\n",
    "    \"Muppets-02-01-01\": 211,\n",
    "    \"Muppets-02-04-04\": 244,\n",
    "    \"Muppets-03-04-03\": 343\n",
    "}\n",
    "\n",
    "# character-oriented: only two\n",
    "SIM1_CHARACTER_LABEL_COLS = [\"Kermit\", \"StatlerWaldorf\", \"Fozzie Bear\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Dataset configuration\n",
    "\n",
    "- Episodes: video paths + GT files + per-episode time split\n",
    "\n",
    "- Mapping episode name → numeric Video id used in GT\n",
    "\n",
    "- Characters for SIM1 (binary presence labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GT combined\n",
    "\n",
    "# Load + consolidate Ground Truth (GT)\n",
    "# Read GT from all episodes and normalize timestamps\n",
    "# Output: one combined dataframe used for feature extraction\n",
    "all_ep_gt_df = prepare_df.all_ep_gt(EPISODES)\n",
    "print(all_ep_gt_df.shape)\n",
    "display(all_ep_gt_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Build audio feature space (frame-aligned)\n",
    "\n",
    "- Extract per-frame audio features (MFCC + deltas, F0, spectral centroid, …)\n",
    "\n",
    "- Align features with GT by (Video, Frame_number, Timestamp)\n",
    "\n",
    "- Save to data/processed/feature_spaces/audio_sim1.csv for reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define character columns for SIM1 audio feature space ---\n",
    "\n",
    "# All characters we want to KEEP in the feature space (GT columns),\n",
    "# even if we do not train models for all of them\n",
    "SIM1_ALL_CHAR_COLS = [\n",
    "    \"Kermit\",\n",
    "    \"StatlerWaldorf\",\n",
    "    \"Fozzie Bear\"\n",
    "]\n",
    "\n",
    "print(\"SIM1_ALL_CHAR_COLS:\", SIM1_ALL_CHAR_COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = audioTools.AudioFrameConfig(\n",
    "    sr=22050,\n",
    "    fps=FPS_TO_SAVE,\n",
    "    n_fft=2048,\n",
    "    n_mfcc=13\n",
    ")\n",
    "\n",
    "audio_sim1 = audioTools.build_audio_feature_space_df(\n",
    "    EPISODES=EPISODES,\n",
    "    EPISODE_NAME_TO_VIDEO_ID=EPISODE_NAME_TO_VIDEO_ID,\n",
    "    gt_df=all_ep_gt_df,\n",
    "    character_cols=SIM1_ALL_CHAR_COLS,\n",
    "    out_csv_path=\"../data/processed/feature_spaces/audio_sim1.csv\",\n",
    "    cache_dir=\"../data/raw/_audio_cache\",\n",
    "    cfg=cfg\n",
    ")\n",
    "\n",
    "display(audio_sim1.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: audio ↔ visual row alignment\n",
    "\n",
    "# verify both modalities contain the same GT-aligned keys\n",
    "# expect large intersection; mismatch indicates FPS / frame extraction issues\n",
    "\n",
    "visual_sim1 = pd.read_csv(\"../data/processed/feature_spaces/visual_sim1.csv\")\n",
    "\n",
    "key_cols = [\"Video\", \"Frame_number\", \"Timestamp\"]\n",
    "merged = visual_sim1[key_cols].merge(audio_sim1[key_cols], on=key_cols, how=\"inner\")\n",
    "print(\"Visual rows:\", len(visual_sim1))\n",
    "print(\"Audio rows:\", len(audio_sim1))\n",
    "print(\"Intersection:\", len(merged))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Train/test split (time-blocked, no leakage) + preprocessing\n",
    "\n",
    "- Split each episode by timestamp (early = train, later = test)\n",
    "\n",
    "- Build X by dropping labels and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Config ---\n",
    "\n",
    "# --- Labels: keep ALL available GT chars in feature space, but train on subset ---\n",
    "# In SIM1 GT may include more characters (e.g., Fozzie Bear). We keep them in the CSV,\n",
    "# but for SIM1 audio we train only on target characters (Kermit + StatlerWaldorf).\n",
    "\n",
    "META_COLS = [\"Video\", \"Frame_number\", \"Timestamp\"]\n",
    "\n",
    "audio_df = pd.read_csv(\"../data/processed/feature_spaces/audio_sim1.csv\")\n",
    "\n",
    "# All character GT columns present in the feature space (exclude meta + feature cols)\n",
    "ALL_SIM1_CHARS_DESIRED = [\"Kermit\", \"StatlerWaldorf\", \"Fozzie Bear\"]  # keep GT for all (if exists)\n",
    "\n",
    "SIM1_ALL_CHAR_COLS = [c for c in ALL_SIM1_CHARS_DESIRED if c in audio_df.columns]\n",
    "print(\"GT columns kept in feature space:\", SIM1_ALL_CHAR_COLS)\n",
    "\n",
    "SIM1_CHARACTER_LABEL_COLS = [c for c in [\"Kermit\", \"StatlerWaldorf\"] if c in audio_df.columns]\n",
    "assert len(SIM1_CHARACTER_LABEL_COLS) > 0, \"No target character label columns found in audio_df.\"\n",
    "print(\"Audio models trained for:\", SIM1_CHARACTER_LABEL_COLS)\n",
    "\n",
    "\n",
    "# --- 1) Split (same logic as visual) ---\n",
    "train_df, test_df = prepare_df.split_feature_space_df(\n",
    "    feature_df=audio_df,\n",
    "    EPISODES=EPISODES,\n",
    "    EPISODE_NAME_TO_VIDEO_ID=EPISODE_NAME_TO_VIDEO_ID\n",
    ")\n",
    "\n",
    "# --- 2) Build X/y ---\n",
    "DROP_COLS = SIM1_CHARACTER_LABEL_COLS + META_COLS\n",
    "X_train_df = train_df.drop(columns=DROP_COLS)\n",
    "X_test_df  = test_df.drop(columns=DROP_COLS)\n",
    "\n",
    "# same column order\n",
    "X_test_df = X_test_df[X_train_df.columns]\n",
    "\n",
    "col_names = X_train_df.columns.tolist()\n",
    "print(\"Training features:\", col_names)\n",
    "\n",
    "# --- 3) Impute (f0 can be NaN) ---\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "X_train = imputer.fit_transform(X_train_df)\n",
    "X_test  = imputer.transform(X_test_df)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test  = scaler.transform(X_test)\n",
    "\n",
    "# --- 5) Train per-character binary model + store BOTH score and hard label ---\n",
    "y_true_df = test_df[SIM1_CHARACTER_LABEL_COLS].copy()\n",
    "y_pred_df = y_true_df.copy()  # will hold *_present and *_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Model 1: SGDClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Train/test split + preprocessing (assumes train_df/test_df already built) ---\n",
    "\n",
    "# Train only on the target characters (but keep all GT cols in the CSV)\n",
    "DROP_COLS = SIM1_ALL_CHAR_COLS + META_COLS  # drop all GT + meta from features\n",
    "\n",
    "X_train_df = train_df.drop(columns=DROP_COLS)\n",
    "X_test_df  = test_df.drop(columns=DROP_COLS)\n",
    "\n",
    "# ensure same column order\n",
    "X_test_df = X_test_df[X_train_df.columns]\n",
    "\n",
    "# --- Impute + scale ---\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "X_train = imputer.fit_transform(X_train_df)\n",
    "X_test  = imputer.transform(X_test_df)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test  = scaler.transform(X_test)\n",
    "\n",
    "# --- y_true + y_pred containers ---\n",
    "y_true_df = test_df[SIM1_CHARACTER_LABEL_COLS].copy()\n",
    "y_pred_df = test_df[META_COLS].copy()  \n",
    "\n",
    "# --- FINAL Model 1: per-character binary classification (SGDClassifier, log-loss) ---\n",
    "for character in SIM1_CHARACTER_LABEL_COLS:\n",
    "    y_train = train_df[character].astype(int).values\n",
    "\n",
    "    clf = SGDClassifier(\n",
    "        loss=\"log_loss\",\n",
    "        penalty=\"l2\",\n",
    "        alpha=1e-4,\n",
    "        max_iter=2000,\n",
    "        tol=1e-3,\n",
    "        random_state=42\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_score = clf.predict_proba(X_test)[:, 1]\n",
    "    y_pred  = (y_score >= 0.5).astype(int)\n",
    "\n",
    "    y_pred_df[f\"{character}_score\"]   = y_score\n",
    "    y_pred_df[f\"{character}_present\"] = y_pred\n",
    "\n",
    "# --- Evaluation (FINAL) ---\n",
    "metrics_sgd, overall_map_sgd = eval.evaluate_multiclass(\n",
    "    y_true_df=y_true_df,\n",
    "    y_pred_df=y_pred_df,\n",
    "    characters=SIM1_CHARACTER_LABEL_COLS\n",
    ")\n",
    "\n",
    "print(\"Overall MAP (SGD FINAL):\", overall_map_sgd)\n",
    "print(metrics_sgd)\n",
    "\n",
    "BEST_AUDIO_MODEL = {\n",
    "    \"name\": \"SGDClassifier(log_loss)\",\n",
    "    \"overall_map\": float(overall_map_sgd),\n",
    "    \"per_character_map\": {k: float(v[\"MAP\"]) for k, v in metrics_sgd.items()}\n",
    "}\n",
    "BEST_AUDIO_MODEL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Model 2: Logistic Regression baseline (probabilistic scores)\n",
    "\n",
    "- Same split + preprocessing\n",
    "\n",
    "- Balanced class weights to address strong label imbalance\n",
    "\n",
    "- Outputs calibrated probabilities via predict_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Baseline Model: Logistic Regression (for comparison only, NOT used for fusion export) ---\n",
    "\n",
    "y_pred_lr = test_df[META_COLS].copy()\n",
    "\n",
    "for character in SIM1_CHARACTER_LABEL_COLS:\n",
    "    y_train = train_df[character].astype(int).values\n",
    "\n",
    "    clf = LogisticRegression(\n",
    "        class_weight=\"balanced\",\n",
    "        max_iter=2000,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_score = clf.predict_proba(X_test)[:, 1]\n",
    "    y_pred  = (y_score >= 0.5).astype(int)\n",
    "\n",
    "    y_pred_lr[f\"{character}_score\"]   = y_score\n",
    "    y_pred_lr[f\"{character}_present\"] = y_pred\n",
    "\n",
    "metrics_lr, overall_map_lr = eval.evaluate_multiclass(\n",
    "    y_true_df=y_true_df,\n",
    "    y_pred_df=y_pred_lr,\n",
    "    characters=SIM1_CHARACTER_LABEL_COLS\n",
    ")\n",
    "\n",
    "print(\"Overall MAP (LogReg baseline):\", overall_map_lr)\n",
    "print(metrics_lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio-based Classification: Discussion and Conclusions\n",
    "\n",
    "In the audio-only part of SIM1, we focused on detecting character presence using classical audio features, including MFCCs with first and second order derivatives, pitch (F0), and spectral centroid. These features are widely used to capture voice timbre, pitch characteristics, and spectral properties of speech.\n",
    "\n",
    "The audio feature space was constructed to include ground truth labels for all characters available in SIM1, even though the audio models were trained only for a selected subset of characters (Kermit and Statler & Waldorf). This design choice ensures consistency of the feature space across modalities and allows seamless integration in later multimodal fusion stages.\n",
    "\n",
    "For classification, the final audio model was implemented using an SGDClassifier with log-loss, which provides probabilistic outputs and scales well to large frame-level datasets. Logistic Regression was used as a baseline for comparison but was not used for downstream fusion.\n",
    "\n",
    "The results show that audio features are moderately effective for detecting Kermit, achieving a Mean Average Precision (MAP) of approximately 0.64. This can be explained by the fact that Kermit has a highly distinctive voice, characterized by a relatively stable pitch range and consistent spectral patterns. As a result, MFCC-based features are able to capture discriminative information that supports reliable detection of his presence.\n",
    "\n",
    "In contrast, the performance for Statler & Waldorf is significantly lower in the audio-only setting, with MAP values around 0.04–0.06. This indicates that audio features alone are not sufficient to reliably detect these characters. Several factors contribute to this limitation. First, Statler and Waldorf frequently speak simultaneously or overlap with laughter, background noise, or other speakers. Second, their vocal characteristics are less distinctive in terms of pitch and timbre compared to Kermit. Finally, the frame-level audio annotations do not always align precisely with actual speaking activity, which introduces additional label noise.\n",
    "\n",
    "Overall, the audio-only results demonstrate that audio features provide useful but incomplete information for character detection. They perform well for characters with strong and consistent vocal signatures, but struggle in scenes with overlapping speech, background noise, or weak audio cues. This confirms the limitations of relying on a single modality and motivates the use of multimodal fusion, where audio cues can complement more reliable visual information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dicom-dev, 3.12)",
   "language": "python",
   "name": "dicom-dev-312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
